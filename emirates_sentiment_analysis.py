# -*- coding: utf-8 -*-
"""Emirates Sentiment analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pnlMXnGpZ2w6FcGone89kEEzZh2kZhOd
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import string
import seaborn as sns
import matplotlib.pyplot as plt

base_url = "https://www.airlinequality.com/airline-reviews/emirates"
pages = 23
page_size = 100

reviews = []

# for i in range(1, pages + 1):
for i in range(1, pages + 1):

    print(f"Scraping page {i}")

    # Create URL to collect links from paginated data
    url = f"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}"

    # Collect HTML data from this page
    response = requests.get(url)

    # Parse content
    content = response.content
    parsed_content = BeautifulSoup(content, 'html.parser')
    for para in parsed_content.find_all("div", {"class": "text_content"}):
        reviews.append(para.get_text())
    
    print(f"   ---> {len(reviews)} total reviews")

df = pd.DataFrame()
df["reviews"] = reviews
df.head()

# write the dataframe to a CSV file 
df.to_csv("e_reviews.csv", index = False)

# Read the CSV file into a pandas DataFrame
reviews = pd.read_csv("e_reviews.csv")

reviews.head()

def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text

# Convert integer values to string
reviews['reviews'] = reviews['reviews'].astype(str)

# Replace specified strings in the DataFrame
reviews['reviews'] = reviews['reviews'].str.replace('Trip Verified |', '')
reviews['reviews'] = reviews['reviews'].str.replace('Not Verified |', '')
reviews['reviews'] = reviews['reviews'].str.replace('âœ…', '')
reviews['reviews'] = reviews['reviews'].str.replace('|', '')

# Remove punctuations from the DataFrame
reviews['reviews'] = reviews['reviews'].apply(remove_punctuations)

#Generate Polarity scores
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sentiment = SentimentIntensityAnalyzer()

reviews['compound'] = [sentiment.polarity_scores(review)['compound'] for review in reviews['reviews']]
reviews['neg'] = [sentiment.polarity_scores(review)['neg'] for review in reviews['reviews']]
reviews['neu'] = [sentiment.polarity_scores(review)['neu'] for review in reviews['reviews']]
reviews['pos'] = [sentiment.polarity_scores(review)['pos'] for review in reviews['reviews']]

reviews.head()

reviews[['compound','neg', 'neu', 'pos']].describe()

# Calculate the number of Positive reviews
num_pos = (reviews['compound']>=0).sum()
num_pos_df = pd.DataFrame({'Number of positive reviews': [num_pos]})
num_pos_df

# Calculate Positive reviews as percentage of total reviews
perc_pos = (reviews['compound']>=0).sum()/reviews['reviews'].count()*100
perc_pos_df = pd.DataFrame({'% positive reviews': [perc_pos]})
perc_pos_df

# Calculate the number of Negative reviews
num_neg = (reviews['compound']<=0).sum()
num_neg_df = pd.DataFrame({'Number of Negative reviews': [num_neg]})
num_neg_df

#Calculate Negative reviews as percentage of total reviews
perc_neg = (reviews['compound']<=0).sum()/reviews['reviews'].count()*100
perc_neg_df = pd.DataFrame({'% Negative reviews': [perc_neg]})
perc_neg_df

#Explore the distribution of compound scores
sns.histplot(reviews['compound'])

#Explore the distribution of Positive scores
sns.histplot(reviews['pos'])

#Explore the distribution of Negative scores
sns.histplot(reviews['neg'])

#create function to be used for Tokenization, removing stop words and stemming
import nltk
nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('english')
def preprocess_text(text):
    tokenized_document = nltk.tokenize.RegexpTokenizer('[a-zA-Z0-9\']+').tokenize(text)
    cleaned_tokens = [word.lower() for word in tokenized_document if word.lower() not in stop_words]
    stemmed_text = [nltk.stem.PorterStemmer().stem(word) for word in cleaned_tokens]
    return stemmed_text

#Create df for Positive and Negative reviews
dfreviews = reviews
dfreviews['processed_review'] = reviews['reviews'].apply(preprocess_text)
positive_reviews = dfreviews.loc[(dfreviews['compound']>0),:]
negative_reviews = dfreviews.loc[(dfreviews['compound']<0),:]

#Visualize top 10 of the most frequent words using freq distribution
#Use list comprehension to create a list of words appearing in positive reviews
pos_tokens = [word for Review in positive_reviews['processed_review'] for word in Review]
from nltk.probability import FreqDist
pos_freqdist = FreqDist(pos_tokens)
pos_freqdist.tabulate(10)

pos_freqdist.plot(10)

#Use list comprehension to create a list of words appearing in neg reviews
neg_tokens = [word for Review in negative_reviews['processed_review'] for word in Review]
neg_freqdist = FreqDist(neg_tokens)
neg_freqdist.tabulate(10)

neg_freqdist.plot(10)

#using wordcloud lib
#Generate word cloud for postive reviews
from wordcloud import WordCloud
wordcloudpos = WordCloud(background_color='white').generate_from_text(' '.join(pos_tokens))
plt.figure(figsize=(12,12))
plt.imshow(wordcloudpos, interpolation='bilinear')
plt.axis('off')
plt.show()

#Generate word cloud for negative reviews
wordcloudneg = WordCloud(background_color='white').generate_from_text(' '.join(neg_tokens))
plt.figure(figsize=(12,12))
plt.imshow(wordcloudneg, interpolation='bilinear')
plt.axis('off')
plt.show()